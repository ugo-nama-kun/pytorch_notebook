{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch basics\n",
    "If you are using jupyterlab, enable \"show contextual help\" for checking the definitions of functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      "torch.Size([3, 4])\n",
      "tensor([[1.7880, 1.6233, 1.7106, 1.1526],\n",
      "        [1.6345, 1.1149, 1.2167, 1.6470],\n",
      "        [1.1962, 1.2656, 1.3082, 1.1896]])\n"
     ]
    }
   ],
   "source": [
    "# Get variables\n",
    "x = torch.ones(3, 4)\n",
    "\n",
    "print(x)\n",
    "print(x.view(1, 12))\n",
    "print(x.view(2, -1))\n",
    "\n",
    "y = torch.rand(3, 4)\n",
    "z = x + y\n",
    "\n",
    "print(z.size())\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.]], requires_grad=True)\n",
      "tensor([[4., 4.]], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x12cb13710>\n",
      "tensor([[48., 48.]], grad_fn=<MulBackward0>) tensor(48., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Auto grad basics\n",
    "x = torch.ones(1, 2, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "y = 2 * x + 2\n",
    "print(y)\n",
    "print(y.grad_fn)\n",
    "\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(z, out)\n",
    "\n",
    "# , detach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x12c7df5f8>\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 2)  # <-- for constants, requires_grad is initially False\n",
    "x = ((x * 3) / (x - 1))\n",
    "print(x.requires_grad)\n",
    "\n",
    "x.requires_grad_(True) # <-- Switch requires_grad by hand\n",
    "print(x.requires_grad)\n",
    "\n",
    "y = (x ** 2).sum()\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "True\n",
      "None\n",
      "---\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "True\n",
      "<AddBackward0 object at 0x12cde6a20>\n",
      "---\n",
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n",
      "True True\n",
      "<MulBackward0 object at 0x12cde6a20> <MeanBackward0 object at 0x12cde6ac8>\n",
      "---\n",
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True) # <-- enable grad calculation\n",
    "print(x)\n",
    "print(x.requires_grad)\n",
    "print(x.grad_fn)\n",
    "print(\"---\")\n",
    "\n",
    "y = x + 2\n",
    "print(y)\n",
    "print(y.requires_grad)\n",
    "print(y.grad_fn)\n",
    "print(\"---\")\n",
    "\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(z, out)\n",
    "print(z.requires_grad, out.requires_grad)\n",
    "print(z.grad_fn, out.grad_fn)\n",
    "print(\"---\")\n",
    "\n",
    "out.backward() # <-- run backprop\n",
    "print(x.grad)\n",
    "\n",
    "# running backward() multiple times elicit an error\n",
    "#out.backward() # <-- run backprop\n",
    "#print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vector-Jacobian product\n",
    "This characteristic of vector-Jacobian product makes it very convenient to feed external gradients into a model that has non-scalar output.\n",
    "\n",
    "Now in this case y is no longer a scalar. torch.autograd could not compute the full Jacobian directly, but if we just want the vector-Jacobian product, simply pass the vector to backward as argument. $J$ is jacobian. and ${\\rm x\\_grad} = J^\\top v$. If the backward target is a scalar function, $v = (\\frac{l}{y_1}, \\cdots, \\frac{l}{y_m})^\\top$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm : 3.464101552963257\n",
      "tensor([2., 2., 2.], grad_fn=<MulBackward0>)\n",
      "norm : 6.928203105926514\n",
      "tensor([4., 4., 4.], grad_fn=<MulBackward0>)\n",
      "norm : 13.856406211853027\n",
      "tensor([8., 8., 8.], grad_fn=<MulBackward0>)\n",
      "norm : 27.712812423706055\n",
      "tensor([16., 16., 16.], grad_fn=<MulBackward0>)\n",
      "norm : 55.42562484741211\n",
      "tensor([32., 32., 32.], grad_fn=<MulBackward0>)\n",
      "norm : 110.85124969482422\n",
      "tensor([64., 64., 64.], grad_fn=<MulBackward0>)\n",
      "norm : 221.70249938964844\n",
      "tensor([128., 128., 128.], grad_fn=<MulBackward0>)\n",
      "norm : 443.4049987792969\n",
      "tensor([256., 256., 256.], grad_fn=<MulBackward0>)\n",
      "norm : 886.8099975585938\n",
      "tensor([512., 512., 512.], grad_fn=<MulBackward0>)\n",
      "norm : 1773.6199951171875\n",
      "tensor([1024., 1024., 1024.], grad_fn=<MulBackward0>)\n",
      "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(3, requires_grad=True)\n",
    "y = x * 2\n",
    "\n",
    "print(f\"norm : {y.data.norm()}\")\n",
    "print(y)\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "    print(f\"norm : {y.data.norm()}\")\n",
    "    print(y)\n",
    "\n",
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(v)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop autograd by wrapping with no_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "# stop autograd and block \n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use detach() to get a \"new\" Tensor with the same content but that does not require gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2], dtype=torch.float, requires_grad=True)\n",
    "y = torch.tensor([3], dtype=torch.float, requires_grad=True)\n",
    "\n",
    "out = (x ** 2) * (y ** 2)\n",
    "print(out.requires_grad)\n",
    "z = out.detach()\n",
    "print(z.requires_grad)\n",
    "\n",
    "# z.backward() # <-- return an error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd and some specific topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Fully Connected Feed Forward NN $f_{\\theta}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.6864, -0.6671, -1.8042, -1.0444,  0.7582])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5)\n",
    "\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Loss $L(\\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate $\\nabla_\\theta L(\\theta, x)|_{x=x_0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate $\\nabla_x L(\\theta, x)|_{x=x_0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update $\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta L(\\theta, x)|_{x=x_0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update weight by minibatch $\\frac{1}{N} \\sum_{n=1}^N \\nabla_\\theta L(\\theta, x)|_{x=x_n}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update $x \\leftarrow x + \\alpha \\nabla_x L(\\theta, x)|_x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy $\\theta_1$ to $\\theta_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy $\\theta_1$ to $\\theta_2$ and update only $\\theta_1$ by $\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta L(\\theta, x)|_{x=x_0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic sampling from $f_\\theta(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inject noise (reparametrization trick) and cal grad $\\nabla_\\theta f_\\theta(x, \\epsilon)$, $\\epsilon\\sim {\\cal N}(\\mu, \\sigma^2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use weighted update $\\theta \\leftarrow \\theta + \\alpha \\delta \\nabla_\\theta L(\\theta, x)|_{x=x_0}$, $\\delta  = f_\\theta(x) - \\theta_0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use noisy weight $f(x, \\theta + \\epsilon)$, $\\epsilon \\sim {\\cal N}(1, 0)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update $\\theta$ by the approximated variational method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define CNN $f_\\theta(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update weight by minibatch for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update $x \\leftarrow x + \\alpha \\nabla_x L(\\theta, x)|_x$ for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define RNN $f_\\theta(x_0, x_1, \\dots, x_T)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update weight by minibatch for RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update $x \\leftarrow x + \\alpha \\nabla_x L(\\theta, x)|_x$ for RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
